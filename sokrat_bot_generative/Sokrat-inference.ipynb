{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bcc0aa-17f2-47fd-b479-fd9f3b0856c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install accelerate\n",
    "!pip install faiss-gpu\n",
    "!pip install trl==0.4.7\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e35c95-d45b-4432-825b-4a1f79a5857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import accelerate\n",
    "import faiss\n",
    "from tqdm.auto import tqdm, trange\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import TrainingArguments, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d4fce8-188e-4582-8923-307909d46f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory\n",
    "os.chdir('D:/sokrat_bot_generative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ed4b3-1e2b-4555-815e-3528ca530e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7749d0-f4b9-4b52-a812-dcd62c66085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA parameters\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "# bitsandbytes parameters\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "# SFT parameters\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790541d-c309-45b2-8281-353596d80853",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab5c62d-b57c-4e61-b9ea-424c07ad763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b7729e-0788-4093-9136-febb9240df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc0159e-b727-4b95-a72b-0749900267f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pretrained model and adapters\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, 'D:/sokrat_bot_generative/checkpoint-500')\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca95848a-787f-40b7-8053-d581a924f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(query, model, tokenizer):\n",
    "\n",
    "    \"\"\"Returns the generated answer \"\"\"\n",
    "\n",
    "    prompt = 'Ниже дан вопрос, дай на него краткий ответ на русском языке\\n\\n'\n",
    "    prompt += f'### Вопрос:\\n{query}\\n'\n",
    "    prompt += f'### Ответ:\\n'\n",
    "    encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    model_inputs = encodeds.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=70, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    answer = ''\n",
    "    answer_ind = 0\n",
    "    answer_2ind = 0\n",
    "    answer_2 = ''\n",
    "    answer_3 = ''\n",
    "    lst = []\n",
    "    for i in decoded[0]:\n",
    "      lst.append(i.strip('/n'))\n",
    "    answer = ''.join(lst)\n",
    "    if ('### Ответ:') in answer:\n",
    "      answer_ind = answer.find('### Ответ:')\n",
    "      answer_2 = answer[answer_ind+11:]\n",
    "      if '### Вопрос:' in answer_2:\n",
    "        answer_2ind = answer_2.find('### Вопрос:')\n",
    "        if '.' in answer_2:\n",
    "          end = answer_2.find('/n')\n",
    "        elif '?' in answer_2:\n",
    "          end = answer_2.find('?')\n",
    "        elif '!' in answer_2:\n",
    "          end = answer_2.find('!')\n",
    "        else:\n",
    "          end = len(answer_2)-1\n",
    "        answer_3 = answer_2[answer_2ind+7:end+1]\n",
    "      else:\n",
    "        answer_3 = answer_2\n",
    "    else:\n",
    "      answer_3 = answer\n",
    "    return answer_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0574a0-22ba-43b9-9788-b00ef4d2084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_result(query):\n",
    "\"\"\"Returns the generated answer for Gradio\"\"\"\n",
    "    \n",
    "  answer_final = get_completion(query, model=llm_model, tokenizer=llm_tokenizer)\n",
    "  return answer_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b2baa7-cab3-410d-912c-b5e5a26c0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=final_result,\n",
    "    inputs=[\"text\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
